{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load groundtruth (single label + multilabel), probabilities, one hot\n",
    "# load as dataframe\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score,f1_score,accuracy_score, precision_score, recall_score, classification_report, roc_auc_score, \\\n",
    "    hamming_loss\n",
    "import numpy as np\n",
    "\n",
    "LABELS_LIST = ['car', 'gym', 'happy', 'night', 'relax',\n",
    "       'running', 'sad', 'summer', 'work', 'workout']\n",
    "\n",
    "# [TODO] edit paths to match audio experiment output\n",
    "exp_dir = \"/src_code/repo/experiments_results/classic_updated_dataset_long/2020-05-08_13-17-43\"\n",
    "our_ground_truth = pd.read_csv(exp_dir+\"/groundtruth_withIDS.csv\")\n",
    "our_predictions=  pd.read_csv(exp_dir+\"/probabilities_withIDS.csv\",)\n",
    "one_hoted_df =  pd.read_csv(exp_dir+\"/one_hoted_withIDS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-output-single-groundtruth Protocol (SO-SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Per label evaluation of single-output-single-groundtruth \n",
    "# Create a dataframe where we keep all the evaluations\n",
    "results_df = pd.DataFrame(columns=LABELS_LIST)\n",
    "results_df.index.astype(str, copy=False)\n",
    "percentage_of_positives_perclass = sum(our_ground_truth.values[:,2:]) / len(our_ground_truth)\n",
    "results_df.loc[0] = percentage_of_positives_perclass\n",
    "results_df.index = ['Ratio of positive samples']\n",
    "\n",
    "# compute additional metrics (AUC,f1,recall,precision)\n",
    "auc_roc_per_label = roc_auc_score(our_ground_truth.values[:,2:], our_predictions.values[:,2:], average=None)\n",
    "precision_perlabel = precision_score(our_ground_truth.values[:,2:], one_hoted_df.values[:,2:], average=None)\n",
    "recall_perlabel = recall_score(our_ground_truth.values[:,2:], one_hoted_df.values[:,2:], average=None)\n",
    "f1_perlabel = f1_score(our_ground_truth.values[:,2:], one_hoted_df.values[:,2:], average=None)\n",
    "\n",
    "results_df = results_df.append(\n",
    "    pd.DataFrame([auc_roc_per_label,recall_perlabel, precision_perlabel, f1_perlabel], columns=LABELS_LIST))\n",
    "results_df.index = ['Ratio of positive samples',\"AUC\", \"Recall\", \"Precision\", \"f1-score\"]\n",
    "results_df['average'] = results_df.mean(numeric_only=True, axis=1)\n",
    "results_df.round(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-output-single-groundtruth Protocol (MO-SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Evaluate on multi-label output and single-label grountruth\n",
    "\"\"\"\n",
    "# Create a dataframe where we keep all the evaluations\n",
    "our_predictions=  pd.read_csv(exp_dir+\"/probabilities_withIDS.csv\",)\n",
    "model_output_rounded = np.round(our_predictions.values[:,2:])\n",
    "model_output_rounded = np.clip(model_output_rounded, 0, 1)\n",
    "results_df = pd.DataFrame(columns=LABELS_LIST)\n",
    "results_df.index.astype(str, copy=False)\n",
    "percentage_of_positives_perclass = sum(our_ground_truth.values[:,2:]) / len(our_ground_truth)\n",
    "results_df.loc[0] = percentage_of_positives_perclass\n",
    "results_df.index = ['Ratio of positive samples']\n",
    "\n",
    "# compute additional metrics (AUC,f1,recall,precision)\n",
    "auc_roc_per_label = roc_auc_score(our_ground_truth.values[:,2:], our_predictions.values[:,2:], average=None)\n",
    "precision_perlabel = precision_score(our_ground_truth.values[:,2:], model_output_rounded, average=None)\n",
    "recall_perlabel = recall_score(our_ground_truth.values[:,2:], model_output_rounded, average=None)\n",
    "f1_perlabel = f1_score(our_ground_truth.values[:,2:], model_output_rounded, average=None)\n",
    "\n",
    "results_df = results_df.append(\n",
    "    pd.DataFrame([auc_roc_per_label,recall_perlabel, precision_perlabel, f1_perlabel], columns=LABELS_LIST))\n",
    "results_df.index = ['Ratio of positive samples',\"AUC\", \"Recall\", \"Precision\", \"f1-score\"]\n",
    "results_df['average'] = results_df.mean(numeric_only=True, axis=1)\n",
    "results_df.round(3).T\n",
    "# get plots of confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
