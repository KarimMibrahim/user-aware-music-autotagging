{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "from sklearn.metrics import cohen_kappa_score,f1_score,accuracy_score, precision_score, recall_score, classification_report, roc_auc_score, \\\n",
    "    hamming_loss\n",
    "\n",
    "\n",
    "# [TODO] edit path to match experiments outputs\n",
    "SOURCE_PATH_embeds = \"/src_code/repo/experiments_results/user_aware_system/2020-10-02_12-09-30\"\n",
    "SOURCE_PATH_auds = \"/src_code/repo/experiments_results/audio_system_single_label/2020-10-02_11-06-43\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read audio model's output and groundtruth\n",
    "test_ground_truth = pd.read_csv(\"/src_code/repo/GroundTruth/test_set.csv\")\n",
    "test_groundtruth_from_model = np.loadtxt(SOURCE_PATH_auds + \"/test_ground_truth_classes.txt\",delimiter=',')\n",
    "user_ids = np.loadtxt(SOURCE_PATH_auds + \"/user_ids.txt\",delimiter=',')\n",
    "track_ids = np.loadtxt(SOURCE_PATH_auds + \"/tracks_ids.txt\",delimiter=',')\n",
    "test_output = np.loadtxt(SOURCE_PATH_auds + \"/predictions.out\",delimiter=',')\n",
    "test_output_one_hot = np.loadtxt(SOURCE_PATH_auds + \"/test_output_one_hot.out\",delimiter=',') ### WHAAAT\n",
    "\n",
    "one_hot_a = test_ground_truth.copy()\n",
    "one_hot_a.song_id = track_ids\n",
    "one_hot_a.user_id = user_ids\n",
    "one_hot_a.iloc[:,2:] = test_output_one_hot\n",
    "\n",
    "preda = test_ground_truth.copy()\n",
    "preda.song_id = track_ids\n",
    "preda.user_id = user_ids\n",
    "preda.iloc[:,2:] = test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user's model's output and groundtruth\n",
    "test_ground_truth = pd.read_csv(\"/src_code/repo/GroundTruth/test_set.csv\")\n",
    "test_groundtruth_from_model = np.loadtxt(SOURCE_PATH_embeds + \"/test_ground_truth_classes.txt\",delimiter=',')\n",
    "user_ids = np.loadtxt(SOURCE_PATH_embeds + \"/user_ids.txt\",delimiter=',')\n",
    "track_ids = np.loadtxt(SOURCE_PATH_embeds + \"/tracks_ids.txt\",delimiter=',')\n",
    "test_output = np.loadtxt(SOURCE_PATH_embeds + \"/predictions.out\",delimiter=',')\n",
    "max_idx = test_output.argmax(axis=1)\n",
    "test_output_one_hot = np.zeros_like(test_output)\n",
    "for counter in range(len(max_idx)):\n",
    "    test_output_one_hot[counter,max_idx[counter]] = 1\n",
    "\n",
    "# Format model output and groundtruth to a dataframe\n",
    "gt = test_ground_truth.copy()\n",
    "gt.song_id = track_ids\n",
    "gt.user_id = user_ids\n",
    "gt.iloc[:,2:] = test_groundtruth_from_model\n",
    "\n",
    "one_hot_u = test_ground_truth.copy()\n",
    "one_hot_u.song_id = track_ids\n",
    "one_hot_u.user_id = user_ids\n",
    "one_hot_u.iloc[:,2:] = test_output_one_hot\n",
    "\n",
    "predu = test_ground_truth.copy()\n",
    "predu.song_id = track_ids\n",
    "predu.user_id = user_ids\n",
    "predu.iloc[:,2:] = test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# create a frame of NAs \n",
    "LABELS_LIST = ['car', 'gym', 'happy', 'night', 'relax',\n",
    "    'running', 'sad', 'summer', 'work', 'workout']\n",
    "users_audio_f1 = pd.DataFrame(gt.user_id.unique(),columns = [\"user_id\"])\n",
    "users_embeds_f1 = pd.DataFrame(gt.user_id.unique(),columns = [\"user_id\"])\n",
    "users_audio_recall = pd.DataFrame(gt.user_id.unique(),columns = [\"user_id\"])\n",
    "users_embeds_recall = pd.DataFrame(gt.user_id.unique(),columns = [\"user_id\"])\n",
    "users_audio_acc = pd.DataFrame(gt.user_id.unique(),columns = [\"user_id\"])\n",
    "users_embeds_acc = pd.DataFrame(gt.user_id.unique(),columns = [\"user_id\"])\n",
    "users_audio_prec = pd.DataFrame(gt.user_id.unique(),columns = [\"user_id\"])\n",
    "users_embeds_prec = pd.DataFrame(gt.user_id.unique(),columns = [\"user_id\"])\n",
    "for x in LABELS_LIST:\n",
    "    users_audio_f1[x] = np.NAN\n",
    "    users_embeds_f1[x] = np.NAN\n",
    "    users_audio_recall[x] = np.NAN\n",
    "    users_embeds_recall[x] = np.NAN\n",
    "    users_audio_acc[x] = np.NAN\n",
    "    users_embeds_acc[x] = np.NAN\n",
    "    users_audio_prec[x] = np.NAN\n",
    "    users_embeds_prec[x] = np.NAN\n",
    "users_audio_f1 = users_audio_f1.set_index(\"user_id\")\n",
    "users_embeds_f1 = users_embeds_f1.set_index(\"user_id\")\n",
    "users_audio_recall = users_audio_recall.set_index(\"user_id\")\n",
    "users_embeds_recall = users_embeds_recall.set_index(\"user_id\")\n",
    "users_audio_acc = users_audio_acc.set_index(\"user_id\")\n",
    "users_embeds_acc = users_embeds_acc.set_index(\"user_id\")\n",
    "users_audio_prec = users_audio_prec.set_index(\"user_id\")\n",
    "users_embeds_prec = users_embeds_prec.set_index(\"user_id\")\n",
    "\n",
    "# compute scores for only present labels\n",
    "for user in users_audio_f1.index:\n",
    "    user_truth = gt[gt.user_id == user]\n",
    "    user_preds_audio = one_hot_a.loc[preda.user_id == user]\n",
    "    user_preds_embeds = one_hot_u.loc[predu.user_id == user]\n",
    "    active_labels_idx = user_truth.sum() > 0\n",
    "    active_labels_idx = active_labels_idx[2:]\n",
    "    active_labels = list(compress(LABELS_LIST, active_labels_idx))\n",
    "    for label in active_labels:\n",
    "        users_audio_f1.loc[user][label] = f1_score(user_truth[label],user_preds_audio[label])\n",
    "        users_embeds_f1.loc[user][label] = f1_score(user_truth[label],user_preds_embeds[label])\n",
    "        users_audio_recall.loc[user][label] = recall_score(user_truth[label],user_preds_audio[label])\n",
    "        users_embeds_recall.loc[user][label] = recall_score(user_truth[label],user_preds_embeds[label])\n",
    "        users_audio_acc.loc[user][label] = accuracy_score(user_truth[label],user_preds_audio[label])\n",
    "        users_embeds_acc.loc[user][label] = accuracy_score(user_truth[label],user_preds_embeds[label])\n",
    "        users_audio_prec.loc[user][label] = precision_score(user_truth[label],user_preds_audio[label])\n",
    "        users_embeds_prec.loc[user][label] = precision_score(user_truth[label],user_preds_embeds[label])\n",
    "        \n",
    "# averaging per user\n",
    "user_audio_f1_mean = users_audio_f1.mean(axis=1)\n",
    "user_embeds_f1_mean = users_embeds_f1.mean(axis=1)\n",
    "\n",
    "users_audio_recall_mean = users_audio_recall.mean(axis=1)\n",
    "user_embeds_recall_mean = users_embeds_recall.mean(axis=1)\n",
    "\n",
    "user_audio_acc_mean = users_audio_acc.mean(axis=1)\n",
    "user_embeds_acc_mean = users_embeds_acc.mean(axis=1)\n",
    "\n",
    "user_audio_prec_mean = users_audio_prec.mean(axis=1)\n",
    "user_embeds_prec_mean = users_embeds_prec.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Audio</th>\n",
       "      <td>0.215615</td>\n",
       "      <td>0.200012</td>\n",
       "      <td>0.308569</td>\n",
       "      <td>0.229246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Audio+User</th>\n",
       "      <td>0.245398</td>\n",
       "      <td>0.230948</td>\n",
       "      <td>0.354487</td>\n",
       "      <td>0.262005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy    Recall  Precision  f1-score\n",
       "Audio       0.215615  0.200012   0.308569  0.229246\n",
       "Audio+User  0.245398  0.230948   0.354487  0.262005"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Audio', 'Audio+User'])\n",
    "results_df.index.astype(str, copy=False)\n",
    "results_df.loc[0] = [user_audio_acc_mean.mean(),user_embeds_acc_mean.mean()]\n",
    "results_df.loc[1] = [users_audio_recall_mean.mean(),user_embeds_recall_mean.mean()]\n",
    "results_df.loc[2] = [user_audio_prec_mean.mean(),user_embeds_prec_mean.mean()]\n",
    "results_df.loc[3] = [user_audio_f1_mean.mean(),user_embeds_f1_mean.mean()]\n",
    "results_df.index = ['Accuracy', 'Recall', 'Precision', 'f1-score']\n",
    "results_df = results_df.T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
