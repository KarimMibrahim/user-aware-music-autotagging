{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun the model on the single-label dataset\n",
    "To evaluate the audio model with the SO-SG and MO-SG protocols, we rerun the multi-label pretrained model on the single-label groundtruth dataset (with track+user labels instead of only accumlated track labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import strftime, localtime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# importing the utility functions defined in utilities.py\n",
    "from utilities import *\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "\n",
    "# [TODO] fix directories\n",
    "SOURCE_PATH = \"/src_code/repo/\"\n",
    "SPECTROGRAMS_PATH = \"/src_code/repo/spectrograms\"\n",
    "OUTPUT_PATH = \"/src_code/repo/experiments_results/\"\n",
    "# [TODO] PATH to the pretrained model\n",
    "extra_exp_dir =   \"/src_code/repo/extra_experiment_results/classic_updated_dataset_long/2020-09-07_09-05-33\"\n",
    "\n",
    "\n",
    "EXPERIMENTNAME = \"Single_label_audio_output\"\n",
    "INPUT_SHAPE = (646, 96, 1)\n",
    "#TODO: fix labels\n",
    "LABELS_LIST = ['car', 'gym', 'happy', 'night', 'relax',\n",
    "       'running', 'sad', 'summer', 'work', 'workout']\n",
    "\n",
    "global_labels = pd.read_csv(\"/src_code/repo/GroundTruth/all_labels_clipped.csv\")\n",
    "train_partial = pd.read_csv(\"/src_code/repo/GroundTruth/train_active_clipped.csv\")\n",
    "POS_WEIGHTS = len(train_partial)/train_partial.sum()[2:]\n",
    "POS_WEIGHTS = [np.float32(x) for x in POS_WEIGHTS]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dataset pipelines\n",
    "def get_labels_py(song_id,user_id):\n",
    "    labels = global_labels[global_labels.song_id == song_id][global_labels.user_id == user_id]\n",
    "    labels = labels.iloc[:, 2:].values.flatten() # TODO: fix this shift in dataframe columns when read\n",
    "    labels = labels.astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def tf_get_labels_py(sample, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[\"song_id\"],sample[\"user_id\"]]\n",
    "        labels = tf.py_func(get_labels_py,\n",
    "                            input_args,\n",
    "                            [tf.float32],\n",
    "                            stateful=False)\n",
    "        res = dict(list(sample.items()) + [(\"binary_label\", labels)])\n",
    "        return res\n",
    "\n",
    "\n",
    "def get_dataset(input_csv, input_shape=INPUT_SHAPE, batch_size=32, shuffle=True,\n",
    "                infinite_generator=True, random_crop=False, cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/\"),\n",
    "                num_parallel_calls=32):\n",
    "    # build dataset from csv file\n",
    "    dataset = dataset_from_csv(input_csv)\n",
    "    # Shuffle data\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100, seed=1, reshuffle_each_iteration=True)\n",
    "\n",
    "    # compute mel spectrogram\n",
    "    dataset = dataset.map(lambda sample: load_spectrogram_tf(sample), num_parallel_calls=1)\n",
    "\n",
    "    # filter out errors\n",
    "    dataset = dataset.filter(lambda sample: tf.logical_not(sample[\"error\"]))\n",
    "\n",
    "    # map dynamic compression\n",
    "    C = 100\n",
    "    dataset = dataset.map(lambda sample: dict(sample, features=tf.log(1 + C * sample[\"features\"])),\n",
    "                          num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # Apply permute dimensions\n",
    "    dataset = dataset.map(lambda sample: dict(sample, features=tf.transpose(sample[\"features\"], perm=[1, 2, 0])),\n",
    "                          num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # Filter by shape (remove badly shaped tensors)\n",
    "    dataset = dataset.filter(lambda sample: check_tensor_shape(sample[\"features\"], input_shape))\n",
    "\n",
    "    # set features shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample,\n",
    "                                              features=set_tensor_shape(sample[\"features\"], input_shape)))\n",
    "\n",
    "    # if cache_dir:\n",
    "    #    os.makedirs(cache_dir, exist_ok=True)\n",
    "    #    dataset = dataset.cache(cache_dir)\n",
    "\n",
    "    dataset = dataset.map(lambda sample: tf_get_labels_py(sample), num_parallel_calls=1)\n",
    "\n",
    "\n",
    "    # set output shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample, binary_label=set_tensor_shape(\n",
    "        sample[\"binary_label\"], (len(LABELS_LIST)))))\n",
    "\n",
    "    if infinite_generator:\n",
    "        # Repeat indefinitly\n",
    "        dataset = dataset.repeat(count=-1)\n",
    "\n",
    "    # Make batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Select only features and annotation\n",
    "    dataset = dataset.map(lambda sample: (\n",
    "    sample[\"features\"], sample[\"binary_label\"],sample[\"song_id\"],sample[\"user_id\"]))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_model(x_input, current_keep_prob, train_phase):\n",
    "    # Define model architecture\n",
    "    # C4_model\n",
    "    x_norm = tf.layers.batch_normalization(x_input, training=train_phase)\n",
    "\n",
    "    with tf.name_scope('CNN_1'):\n",
    "        conv1 = conv_layer_with_relu(x_norm, [3, 3, 1, 32], name=\"conv_1\")\n",
    "        max1 = max_pooling(conv1, shape=[1, 2, 2, 1], name=\"max_pool_1\")\n",
    "\n",
    "    with tf.name_scope('CNN_2'):\n",
    "        conv2 = conv_layer_with_relu(max1, [3, 3, 32, 64], name=\"conv_2\")\n",
    "        max2 = max_pooling(conv2, shape=[1, 2, 2, 1], name=\"max_pool_2\")\n",
    "\n",
    "    with tf.name_scope('CNN_3'):\n",
    "        conv3 = conv_layer_with_relu(max2, [3, 3, 64, 128], name=\"conv_3\")\n",
    "        max3 = max_pooling(conv3, shape=[1, 2, 2, 1], name=\"max_pool_3\")\n",
    "\n",
    "    with tf.name_scope('CNN_4'):\n",
    "        conv4 = conv_layer_with_relu(max3, [3, 3, 128, 256], name=\"conv_4\")\n",
    "        max4 = max_pooling(conv4, shape=[1, 2, 2, 1], name=\"max_pool_4\")\n",
    "\n",
    "    with tf.name_scope('Fully_connected_1'):\n",
    "        flattened = tf.reshape(max4, [-1, 41 * 6 * 256])\n",
    "        fully1 = tf.nn.sigmoid(full_layer(flattened, 256))\n",
    "\n",
    "\n",
    "    with tf.name_scope('Fully_connected_2'):\n",
    "        dropped = tf.nn.dropout(fully1, keep_prob=current_keep_prob)\n",
    "        logits = full_layer(dropped, len(LABELS_LIST))\n",
    "\n",
    "    output = tf.nn.sigmoid(logits)\n",
    "    tf.summary.histogram('outputs', output)\n",
    "    return logits, output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Run the main loop\n",
    "\"\"\"\n",
    "print(\"Current Experiment: \" + EXPERIMENTNAME + \"\\n\\n\\n\")\n",
    "# Loading datasets\n",
    "# TODO: fix directories\n",
    "\n",
    "# Setting up model\n",
    "y = tf.placeholder(tf.float32, [None, len(LABELS_LIST)], name=\"true_labels\")\n",
    "x_input = tf.placeholder(tf.float32, [None, 646, 96, 1], name=\"input\")\n",
    "current_keep_prob = tf.placeholder(tf.float32, name=\"dropout_rate\")\n",
    "weights = tf.constant(POS_WEIGHTS)\n",
    "train_phase = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "logits, model_output = get_model(x_input,current_keep_prob, train_phase)\n",
    "\n",
    "# Defining loss and metrics\n",
    "#loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "'''\n",
    "These following lines are needed for batch normalization to work properly\n",
    "check https://timodenk.com/blog/tensorflow-batch-normalization/\n",
    "'''\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#with tf.control_dependencies(update_ops):\n",
    "#    train_step = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "# Setting up saving directory\n",
    "experiment_name = strftime(\"%Y-%m-%d_%H-%M-%S\", localtime())\n",
    "exp_dir = os.path.join(OUTPUT_PATH, EXPERIMENTNAME, experiment_name)\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Loading model with best validation\n",
    "    saver.restore(sess, os.path.join(extra_exp_dir, \"best_validation.ckpt\"))\n",
    "    print(\"Model with best validation restored before testing.\")\n",
    "\n",
    "    test_labels = pd.read_csv(os.path.join(SOURCE_PATH, \"GroundTruth/test_active_clipped.csv\"))\n",
    "    test_dataset = get_dataset(os.path.join(SOURCE_PATH, \"GroundTruth/test_active_clipped.csv\"),shuffle = False)\n",
    "    test_classes = np.zeros_like(test_labels.iloc[:, 2:].values, dtype=float)\n",
    "    # test_images, test_classes = load_test_set_raw(test_split)\n",
    "\n",
    "    TEST_NUM_STEPS = int(np.floor((len(test_classes) / 32)))\n",
    "    # split_size = int(len(test_classes) / TEST_NUM_STEPS)\n",
    "    test_pred_prob = np.zeros_like(test_classes, dtype=float)\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    test_next_element = test_iterator.get_next()\n",
    "    test_song_ids = np.zeros([test_classes.shape[0],1])\n",
    "    test_user_ids = np.zeros([test_classes.shape[0],1])\n",
    "\n",
    "    for test_batch_counter in range(TEST_NUM_STEPS):\n",
    "        start_idx = (test_batch_counter * BATCH_SIZE)\n",
    "        end_idx = (test_batch_counter * BATCH_SIZE) + BATCH_SIZE\n",
    "        test_batch = sess.run(test_next_element)\n",
    "        test_batch_images = test_batch[0]\n",
    "        test_batch_labels = np.squeeze(test_batch[1])\n",
    "        test_song_ids[start_idx:end_idx] = test_batch[2].reshape([-1, 1])\n",
    "        test_user_ids[start_idx:end_idx] = test_batch[3].reshape([-1, 1])\n",
    "        test_classes[start_idx:end_idx, :] = test_batch_labels\n",
    "        test_pred_prob[start_idx:end_idx, :] = sess.run(model_output,\n",
    "                                                        feed_dict={x_input: test_batch_images,\n",
    "                                                                   current_keep_prob: 1.0,\n",
    "                                                                   train_phase: False})\n",
    "\n",
    "    np.savetxt(os.path.join(exp_dir, 'tracks_ids.txt'), test_song_ids, delimiter=',')\n",
    "    np.savetxt(os.path.join(exp_dir, 'user_ids.txt'), test_user_ids, delimiter=',')\n",
    "    accuracy_out, auc_roc, hamming_error = evaluate_model(test_pred_prob, test_classes,\n",
    "                                                          saving_path=exp_dir,\n",
    "                                                          evaluation_file_path= \\\n",
    "                                                              os.path.join(exp_dir, \"evaluation_results.txt\"))\n",
    "    results = create_analysis_report(test_pred_prob, test_classes, exp_dir, LABELS_LIST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
